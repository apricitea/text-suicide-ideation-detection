{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5afdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Data Preps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Stopwords removal and stemming\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from hunspell import Hunspell\n",
    "\n",
    "# Oversasmpling\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# FastText\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# LSTM with keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12bcdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filepath = \"../dataset/final_dataset.xlsx\"\n",
    "\n",
    "df = pd.read_excel(filepath, sheet_name=\"10k\")\n",
    "X = df['tweet']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c71b9",
   "metadata": {},
   "source": [
    "# Import Pre-trained fastText model on Indonesian language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7451456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained fasttext model on Indonesia language\n",
    "filepath = \"../../fasttext_pretrained_model_indonesia/cc.id.300.bin\"\n",
    "\n",
    "fasttext_model = fasttext.load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ab515",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba4f5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].str.lower()\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f2c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_symbols(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(lambda text: re.sub(r'[^\\x00-\\x7F]+', '', text))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_punctuation_and_sc(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_mentions_hashtags(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(lambda x: re.sub(r'@\\w+|\\#\\w+', '', x))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_hyperlink(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_redundant_whitespace(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].str.replace(r'\\s+', ' ', regex=False).str.strip()\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce2fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()\n",
    "\n",
    "def do_stopwords_sastrawi(text):\n",
    "    words = text.split()\n",
    "    words_filtered = [word for word in words if not word in stopwords_sastrawi]\n",
    "    return \" \".join(words_filtered)\n",
    "\n",
    "def remove_stopwords(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(do_stopwords_sastrawi)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328c896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Indonesian stopwords from Hunspell\n",
    "current_directory = os.getcwd()\n",
    "filepath = os.path.join(current_directory, \"..\", \"hunspell-id-main\", \"id_ID\")\n",
    "\n",
    "h = Hunspell(filepath,filepath)\n",
    "\n",
    "def word_hunspell(word):\n",
    "    try:\n",
    "        stems = h.stem(word)\n",
    "    except UnicodeEncodeError:\n",
    "        stems = [word]\n",
    "    \n",
    "    if len(stems) == 0:\n",
    "        output = word\n",
    "    else:\n",
    "        output = stems[0]\n",
    "    return output\n",
    "\n",
    "def stem_hunspell(text):\n",
    "    hs_stem = [word_hunspell(word) for word in text.split()]\n",
    "    output = ' '.join(hs_stem) \n",
    "    return output\n",
    "\n",
    "def stemming(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(stem_hunspell)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c07963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, column_name):  \n",
    "    new_df = df.copy()\n",
    "    new_df = remove_mentions_hashtags(new_df, column_name)\n",
    "    new_df = remove_hyperlink(new_df, column_name)\n",
    "    new_df = remove_punctuation_and_sc(new_df, column_name)\n",
    "    new_df = case_folding(new_df, column_name)\n",
    "    new_df = remove_stopwords(new_df, column_name)\n",
    "    new_df = remove_redundant_whitespace(new_df, column_name)\n",
    "    new_df = stemming(new_df, column_name)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022f9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cf = case_folding(df, 'tweet')\n",
    "#df_rsw = remove_stopwords(df, 'tweet')\n",
    "#df_stem = stemming(df, 'tweet')\n",
    "#df_rpunc = remove_punctuation(df, 'tweet')\n",
    "#df_rsym = remove_symbols(df, 'tweet')\n",
    "#df_rpsc = remove_punctuation_and_sc(df, 'tweet')\n",
    "#df_rrw = remove_redundant_whitespace(df, 'tweet')\n",
    "#df_rmh = remove_mentions_hashtags(df, 'tweet')\n",
    "#df_rhl = remove_hyperlink(df, 'tweet')\n",
    "#df_preprocessed = preprocessing(df, 'tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14661190",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ec37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing tweets and label for each respective preprocessing steps (to be compared by using evaluation metrics)\n",
    "\n",
    "#X = df['tweet']\n",
    "#y = df['label']\n",
    "\n",
    "#X = df_cf['tweet']\n",
    "#y = df_cf['label']\n",
    "\n",
    "#X = df_rsw['tweet']\n",
    "#y = df_rsw['label']\n",
    "\n",
    "#X = df_stem['tweet']\n",
    "#y = df_stem['label']\n",
    "\n",
    "#X = df_rpunc['tweet']\n",
    "#y = df_rpunc['label']\n",
    "\n",
    "#X = df_rsym['tweet']\n",
    "#y = df_rsym['label']\n",
    "\n",
    "#X = df_rpsc['tweet']\n",
    "#y = df_rpsc['label']\n",
    "\n",
    "#X = df_rrw['tweet']\n",
    "#y = df_rrw['label']\n",
    "\n",
    "#X = df_rmh['tweet']\n",
    "#y = df_rmh['label']\n",
    "\n",
    "#X = df_rhl['tweet']\n",
    "#y = df_rhl['label']\n",
    "\n",
    "#X = df_preprocessed['tweet']\n",
    "#y = df_preprocessed['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eaa751",
   "metadata": {},
   "source": [
    "## No Imbalanced Class Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ff80f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1350ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c11cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "max_length = max([len(s.split()) for s in X])\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3c65018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimension of the embeddings\n",
    "dim = fasttext_model.get_dimension()\n",
    "\n",
    "# create the embedding matrix\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = fasttext_model.get_word_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84572949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002437</td>\n",
       "      <td>-0.043058</td>\n",
       "      <td>-0.018707</td>\n",
       "      <td>0.128495</td>\n",
       "      <td>-0.018542</td>\n",
       "      <td>-0.101053</td>\n",
       "      <td>-0.034554</td>\n",
       "      <td>0.143302</td>\n",
       "      <td>-0.066489</td>\n",
       "      <td>-0.113288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069537</td>\n",
       "      <td>-0.042619</td>\n",
       "      <td>-0.024802</td>\n",
       "      <td>-0.008546</td>\n",
       "      <td>-0.021058</td>\n",
       "      <td>-0.034511</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>-0.020076</td>\n",
       "      <td>-0.057329</td>\n",
       "      <td>0.047373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006226</td>\n",
       "      <td>-0.039082</td>\n",
       "      <td>-0.188137</td>\n",
       "      <td>0.121136</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.017565</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>0.010326</td>\n",
       "      <td>-0.003909</td>\n",
       "      <td>-0.072086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>-0.125213</td>\n",
       "      <td>0.051104</td>\n",
       "      <td>-0.055601</td>\n",
       "      <td>-0.034176</td>\n",
       "      <td>-0.043827</td>\n",
       "      <td>-0.025502</td>\n",
       "      <td>-0.045835</td>\n",
       "      <td>-0.027698</td>\n",
       "      <td>0.121965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.051436</td>\n",
       "      <td>-0.028671</td>\n",
       "      <td>-0.022541</td>\n",
       "      <td>0.191665</td>\n",
       "      <td>-0.029133</td>\n",
       "      <td>-0.184469</td>\n",
       "      <td>0.045916</td>\n",
       "      <td>0.121244</td>\n",
       "      <td>0.051155</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032695</td>\n",
       "      <td>-0.007114</td>\n",
       "      <td>0.101565</td>\n",
       "      <td>0.101271</td>\n",
       "      <td>0.069635</td>\n",
       "      <td>-0.078312</td>\n",
       "      <td>0.066119</td>\n",
       "      <td>-0.240654</td>\n",
       "      <td>-0.107772</td>\n",
       "      <td>0.228045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039191</td>\n",
       "      <td>-0.041498</td>\n",
       "      <td>-0.063466</td>\n",
       "      <td>0.100154</td>\n",
       "      <td>-0.048755</td>\n",
       "      <td>-0.259327</td>\n",
       "      <td>-0.061933</td>\n",
       "      <td>-0.004503</td>\n",
       "      <td>0.026468</td>\n",
       "      <td>-0.137056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026037</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.079056</td>\n",
       "      <td>-0.005207</td>\n",
       "      <td>0.022232</td>\n",
       "      <td>-0.019059</td>\n",
       "      <td>-0.042259</td>\n",
       "      <td>-0.080854</td>\n",
       "      <td>-0.021680</td>\n",
       "      <td>0.169146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23680</th>\n",
       "      <td>0.011723</td>\n",
       "      <td>0.061613</td>\n",
       "      <td>-0.031122</td>\n",
       "      <td>0.046441</td>\n",
       "      <td>0.031312</td>\n",
       "      <td>-0.056365</td>\n",
       "      <td>-0.029583</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.030902</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008631</td>\n",
       "      <td>-0.019576</td>\n",
       "      <td>-0.059615</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>-0.001603</td>\n",
       "      <td>-0.054475</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.052755</td>\n",
       "      <td>-0.002003</td>\n",
       "      <td>0.057023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23681</th>\n",
       "      <td>-0.025238</td>\n",
       "      <td>0.014615</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.011921</td>\n",
       "      <td>-0.003852</td>\n",
       "      <td>-0.050046</td>\n",
       "      <td>-0.008946</td>\n",
       "      <td>-0.045779</td>\n",
       "      <td>-0.030470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029670</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>-0.035793</td>\n",
       "      <td>-0.002349</td>\n",
       "      <td>0.016751</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.027547</td>\n",
       "      <td>-0.018048</td>\n",
       "      <td>-0.048202</td>\n",
       "      <td>-0.011137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23682</th>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.030182</td>\n",
       "      <td>0.025152</td>\n",
       "      <td>0.030010</td>\n",
       "      <td>-0.036651</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>-0.007530</td>\n",
       "      <td>-0.041400</td>\n",
       "      <td>0.014827</td>\n",
       "      <td>-0.066604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016342</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>-0.002522</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>-0.007912</td>\n",
       "      <td>-0.007289</td>\n",
       "      <td>0.048508</td>\n",
       "      <td>-0.065911</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.013772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23683</th>\n",
       "      <td>-0.024868</td>\n",
       "      <td>0.088420</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.129497</td>\n",
       "      <td>-0.136866</td>\n",
       "      <td>0.021089</td>\n",
       "      <td>-0.103722</td>\n",
       "      <td>0.031331</td>\n",
       "      <td>0.137365</td>\n",
       "      <td>-0.115415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090035</td>\n",
       "      <td>0.075169</td>\n",
       "      <td>-0.143936</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>-0.108176</td>\n",
       "      <td>0.055342</td>\n",
       "      <td>-0.028520</td>\n",
       "      <td>-0.065496</td>\n",
       "      <td>0.151506</td>\n",
       "      <td>0.125149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23684</th>\n",
       "      <td>-0.010866</td>\n",
       "      <td>-0.019881</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.065581</td>\n",
       "      <td>0.030249</td>\n",
       "      <td>-0.048798</td>\n",
       "      <td>-0.018606</td>\n",
       "      <td>0.029834</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>-0.098323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023642</td>\n",
       "      <td>0.033683</td>\n",
       "      <td>-0.035517</td>\n",
       "      <td>0.008968</td>\n",
       "      <td>0.039330</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.024102</td>\n",
       "      <td>-0.136492</td>\n",
       "      <td>-0.015498</td>\n",
       "      <td>0.036063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23685 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1      0.002437 -0.043058 -0.018707  0.128495 -0.018542 -0.101053 -0.034554   \n",
       "2      0.006226 -0.039082 -0.188137  0.121136 -0.001375 -0.017565  0.021986   \n",
       "3      0.051436 -0.028671 -0.022541  0.191665 -0.029133 -0.184469  0.045916   \n",
       "4     -0.039191 -0.041498 -0.063466  0.100154 -0.048755 -0.259327 -0.061933   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23680  0.011723  0.061613 -0.031122  0.046441  0.031312 -0.056365 -0.029583   \n",
       "23681 -0.025238  0.014615  0.002253  0.030075  0.011921 -0.003852 -0.050046   \n",
       "23682 -0.011442 -0.030182  0.025152  0.030010 -0.036651  0.007234 -0.007530   \n",
       "23683 -0.024868  0.088420  0.243243  0.129497 -0.136866  0.021089 -0.103722   \n",
       "23684 -0.010866 -0.019881  0.050228  0.065581  0.030249 -0.048798 -0.018606   \n",
       "\n",
       "            7         8         9    ...       290       291       292  \\\n",
       "0      0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1      0.143302 -0.066489 -0.113288  ... -0.069537 -0.042619 -0.024802   \n",
       "2      0.010326 -0.003909 -0.072086  ...  0.007077 -0.125213  0.051104   \n",
       "3      0.121244  0.051155 -0.261428  ...  0.032695 -0.007114  0.101565   \n",
       "4     -0.004503  0.026468 -0.137056  ... -0.026037  0.006308  0.079056   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23680  0.016447  0.030902  0.011736  ...  0.008631 -0.019576 -0.059615   \n",
       "23681 -0.008946 -0.045779 -0.030470  ... -0.029670  0.030701 -0.035793   \n",
       "23682 -0.041400  0.014827 -0.066604  ... -0.016342  0.007197 -0.002522   \n",
       "23683  0.031331  0.137365 -0.115415  ... -0.090035  0.075169 -0.143936   \n",
       "23684  0.029834  0.007224 -0.098323  ... -0.023642  0.033683 -0.035517   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1     -0.008546 -0.021058 -0.034511  0.009614 -0.020076 -0.057329  0.047373  \n",
       "2     -0.055601 -0.034176 -0.043827 -0.025502 -0.045835 -0.027698  0.121965  \n",
       "3      0.101271  0.069635 -0.078312  0.066119 -0.240654 -0.107772  0.228045  \n",
       "4     -0.005207  0.022232 -0.019059 -0.042259 -0.080854 -0.021680  0.169146  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "23680  0.000401 -0.001603 -0.054475  0.006014  0.052755 -0.002003  0.057023  \n",
       "23681 -0.002349  0.016751  0.002381  0.027547 -0.018048 -0.048202 -0.011137  \n",
       "23682 -0.029678 -0.007912 -0.007289  0.048508 -0.065911  0.002160  0.013772  \n",
       "23683  0.024632 -0.108176  0.055342 -0.028520 -0.065496  0.151506  0.125149  \n",
       "23684  0.008968  0.039330  0.003242  0.024102 -0.136492 -0.015498  0.036063  \n",
       "\n",
       "[23685 rows x 300 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "764f1c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\apeir\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\text-suicide-ideation-detection-yF477YnD-py3.9\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\apeir\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\text-suicide-ideation-detection-yF477YnD-py3.9\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\apeir\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\text-suicide-ideation-detection-yF477YnD-py3.9\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "102/102 [==============================] - 11s 85ms/step - loss: 0.4639 - val_loss: 0.3843\n",
      "Epoch 2/10\n",
      "102/102 [==============================] - 10s 94ms/step - loss: 0.3349 - val_loss: 0.3206\n",
      "Epoch 3/10\n",
      "102/102 [==============================] - 10s 99ms/step - loss: 0.3028 - val_loss: 0.3185\n",
      "Epoch 4/10\n",
      "102/102 [==============================] - 10s 99ms/step - loss: 0.2787 - val_loss: 0.3194\n",
      "Epoch 5/10\n",
      "102/102 [==============================] - 10s 100ms/step - loss: 0.2600 - val_loss: 0.2770\n",
      "Epoch 6/10\n",
      "102/102 [==============================] - 10s 100ms/step - loss: 0.2482 - val_loss: 0.2658\n",
      "Epoch 7/10\n",
      "102/102 [==============================] - 10s 101ms/step - loss: 0.2380 - val_loss: 0.2832\n",
      "Epoch 8/10\n",
      "102/102 [==============================] - 10s 101ms/step - loss: 0.2363 - val_loss: 0.3152\n",
      "Epoch 9/10\n",
      "102/102 [==============================] - 10s 100ms/step - loss: 0.2271 - val_loss: 0.2676\n",
      "Epoch 10/10\n",
      "102/102 [==============================] - 10s 101ms/step - loss: 0.2161 - val_loss: 0.2577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x240d624a6d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d545c259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 10ms/step\n",
      "[[1551   83]\n",
      " [  98  294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94      1634\n",
      "           1       0.78      0.75      0.76       392\n",
      "\n",
      "    accuracy                           0.91      2026\n",
      "   macro avg       0.86      0.85      0.85      2026\n",
      "weighted avg       0.91      0.91      0.91      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2c9fede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 3s 10ms/step\n",
      "[[6298  229]\n",
      " [ 422 1153]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      6527\n",
      "           1       0.83      0.73      0.78      1575\n",
      "\n",
      "    accuracy                           0.92      8102\n",
      "   macro avg       0.89      0.85      0.87      8102\n",
      "weighted avg       0.92      0.92      0.92      8102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66eae9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apeir\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\text-suicide-ideation-detection-yF477YnD-py3.9\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_name = './suicide_detection_no_treatment.h5'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bb135",
   "metadata": {},
   "source": [
    "# Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70614288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels and their frequencies in the training data\n",
    "class_labels = np.unique(y_train)\n",
    "class_freq = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train)\n",
    "\n",
    "# Calculate the inverse frequency as the class weight\n",
    "class_weight = dict(zip(class_labels, class_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8afe9fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "102/102 [==============================] - 14s 107ms/step - loss: 0.5535 - val_loss: 0.4558\n",
      "Epoch 2/10\n",
      "102/102 [==============================] - 11s 104ms/step - loss: 0.4280 - val_loss: 0.5752\n",
      "Epoch 3/10\n",
      "102/102 [==============================] - 11s 106ms/step - loss: 0.3855 - val_loss: 0.4056\n",
      "Epoch 4/10\n",
      "102/102 [==============================] - 11s 108ms/step - loss: 0.3505 - val_loss: 0.4096\n",
      "Epoch 5/10\n",
      "102/102 [==============================] - 11s 106ms/step - loss: 0.3477 - val_loss: 0.3977\n",
      "Epoch 6/10\n",
      "102/102 [==============================] - 11s 108ms/step - loss: 0.3252 - val_loss: 0.3452\n",
      "Epoch 7/10\n",
      "102/102 [==============================] - 11s 107ms/step - loss: 0.3157 - val_loss: 0.3741\n",
      "Epoch 8/10\n",
      "102/102 [==============================] - 11s 106ms/step - loss: 0.3066 - val_loss: 0.4208\n",
      "Epoch 9/10\n",
      "102/102 [==============================] - 11s 107ms/step - loss: 0.3032 - val_loss: 0.2972\n",
      "Epoch 10/10\n",
      "102/102 [==============================] - 11s 107ms/step - loss: 0.2857 - val_loss: 0.2765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x240d9bc7040>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae439115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 10ms/step\n",
      "[[1492  142]\n",
      " [  72  320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      1634\n",
      "           1       0.69      0.82      0.75       392\n",
      "\n",
      "    accuracy                           0.89      2026\n",
      "   macro avg       0.82      0.86      0.84      2026\n",
      "weighted avg       0.90      0.89      0.90      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f21d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 3s 10ms/step\n",
      "[[6095  432]\n",
      " [ 264 1311]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95      6527\n",
      "           1       0.75      0.83      0.79      1575\n",
      "\n",
      "    accuracy                           0.91      8102\n",
      "   macro avg       0.86      0.88      0.87      8102\n",
      "weighted avg       0.92      0.91      0.92      8102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecd55dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apeir\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\text-suicide-ideation-detection-yF477YnD-py3.9\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_name = './suicide_detection_cw_final.h5'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1077f",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bde87",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cf44b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "adasyn = ADASYN()\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7015810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 20s 109ms/step - loss: 0.5680 - val_loss: 0.6606\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 18s 109ms/step - loss: 0.4573 - val_loss: 1.0240\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 14s 89ms/step - loss: 0.4235 - val_loss: 0.6154\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 14s 87ms/step - loss: 0.3930 - val_loss: 0.4192\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 14s 86ms/step - loss: 0.3686 - val_loss: 0.5737\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 14s 85ms/step - loss: 0.3539 - val_loss: 0.4893\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 14s 86ms/step - loss: 0.3352 - val_loss: 0.3843\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 14s 85ms/step - loss: 0.3217 - val_loss: 0.6602\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 14s 85ms/step - loss: 0.3081 - val_loss: 0.5135\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 14s 86ms/step - loss: 0.2989 - val_loss: 0.5937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x240fe1a0a90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.fit(X_adasyn, y_adasyn, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0252dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 8ms/step\n",
      "[[1490  144]\n",
      " [ 102  290]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      1634\n",
      "           1       0.67      0.74      0.70       392\n",
      "\n",
      "    accuracy                           0.88      2026\n",
      "   macro avg       0.80      0.83      0.81      2026\n",
      "weighted avg       0.88      0.88      0.88      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "113c028a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/254 [..............................] - ETA: 5s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 2s 8ms/step\n",
      "[[6265  262]\n",
      " [ 440 1135]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95      6527\n",
      "           1       0.81      0.72      0.76      1575\n",
      "\n",
      "    accuracy                           0.91      8102\n",
      "   macro avg       0.87      0.84      0.86      8102\n",
      "weighted avg       0.91      0.91      0.91      8102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49e52a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apeir\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\text-suicide-ideation-detection-yF477YnD-py3.9\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_name = './suicide_detection_adasyn.h5'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf46080",
   "metadata": {},
   "source": [
    "### SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a161b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoteenn = SMOTEENN()\n",
    "X_smoteenn, y_smoteenn = smoteenn.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c10644c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117/117 [==============================] - 13s 92ms/step - loss: 0.5589 - val_loss: 0.5800\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 10s 89ms/step - loss: 0.4443 - val_loss: 0.3770\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 10s 90ms/step - loss: 0.3914 - val_loss: 0.5205\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 11s 90ms/step - loss: 0.3688 - val_loss: 0.5025\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 11s 91ms/step - loss: 0.3410 - val_loss: 0.8067\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 11s 91ms/step - loss: 0.3313 - val_loss: 0.4556\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 11s 91ms/step - loss: 0.3087 - val_loss: 0.2950\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 11s 91ms/step - loss: 0.2872 - val_loss: 0.1909\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 11s 91ms/step - loss: 0.2745 - val_loss: 0.2370\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 11s 92ms/step - loss: 0.2596 - val_loss: 0.2649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x242b0bf21c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.fit(X_smoteenn, y_smoteenn, epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e7471c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 9ms/step\n",
      "[[1202  432]\n",
      " [  46  346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.83      1634\n",
      "           1       0.44      0.88      0.59       392\n",
      "\n",
      "    accuracy                           0.76      2026\n",
      "   macro avg       0.70      0.81      0.71      2026\n",
      "weighted avg       0.86      0.76      0.79      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bf27fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/254 [..............................] - ETA: 6s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 2s 9ms/step\n",
      "[[5460 1067]\n",
      " [ 162 1413]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90      6527\n",
      "           1       0.57      0.90      0.70      1575\n",
      "\n",
      "    accuracy                           0.85      8102\n",
      "   macro avg       0.77      0.87      0.80      8102\n",
      "weighted avg       0.89      0.85      0.86      8102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_train, y_pred))\n",
    "print(classification_report(y_train, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
