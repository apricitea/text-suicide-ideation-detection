{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5afdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\LENOVO\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\text-suicide-ideation-detection-yF477YnD-py3.9\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Data Preps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Stopwords removal and stemming\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from hunspell import Hunspell\n",
    "\n",
    "# Oversasmpling\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# FastText\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# LSTM with keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12bcdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filepath = \"../dataset/final_dataset.xlsx\"\n",
    "\n",
    "df = pd.read_excel(filepath, sheet_name=\"10k\")\n",
    "X = df['tweet']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c71b9",
   "metadata": {},
   "source": [
    "# Import Pre-trained fastText model on Indonesian language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7451456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained fasttext model on Indonesia language\n",
    "filepath = \"../../fasttext_pretrained_model_indonesia/cc.id.300.bin\"\n",
    "\n",
    "fasttext_model = fasttext.load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ab515",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f2c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].str.lower()\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_punctuation(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_symbols(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(lambda text: re.sub(r'[^\\x00-\\x7F]+', '', text))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_punctuation_and_sc(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_mentions_hashtags(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(lambda x: re.sub(r'@\\w+|\\#\\w+', '', x))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_hyperlink(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def remove_redundant_whitespace(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].str.replace(r'\\s+', ' ', regex=False).str.strip()\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce2fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()\n",
    "\n",
    "def do_stopwords_sastrawi(text):\n",
    "    words = text.split()\n",
    "    words_filtered = [word for word in words if not word in stopwords_sastrawi]\n",
    "    return \" \".join(words_filtered)\n",
    "\n",
    "def remove_stopwords(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(do_stopwords_sastrawi)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328c896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Indonesian stopwords from Hunspell\n",
    "current_directory = os.getcwd()\n",
    "filepath = os.path.join(current_directory, \"..\", \"hunspell-id-main\", \"id_ID\")\n",
    "\n",
    "h = Hunspell(filepath,filepath)\n",
    "\n",
    "def word_hunspell(word):\n",
    "    try:\n",
    "        stems = h.stem(word)\n",
    "    except UnicodeEncodeError:\n",
    "        stems = [word]\n",
    "    \n",
    "    if len(stems) == 0:\n",
    "        output = word\n",
    "    else:\n",
    "        output = stems[0]\n",
    "    return output\n",
    "\n",
    "def stem_hunspell(text):\n",
    "    hs_stem = [word_hunspell(word) for word in text.split()]\n",
    "    output = ' '.join(hs_stem) \n",
    "    return output\n",
    "\n",
    "def stemming(df, column_name):\n",
    "    new_df = df.copy()\n",
    "    new_df[column_name] = new_df[column_name].apply(stem_hunspell)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c07963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, column_name):  \n",
    "    new_df = df.copy()\n",
    "    new_df = remove_mentions_hashtags(new_df, column_name)\n",
    "    new_df = remove_hyperlink(new_df, column_name)\n",
    "    new_df = remove_punctuation_and_sc(new_df, column_name)\n",
    "    new_df = case_folding(new_df, column_name)\n",
    "    new_df = remove_stopwords(new_df, column_name)\n",
    "    new_df = remove_redundant_whitespace(new_df, column_name)\n",
    "    new_df = stemming(new_df, column_name)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022f9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cf = case_folding(df, 'tweet')\n",
    "#df_rsw = remove_stopwords(df, 'tweet')\n",
    "#df_stem = stemming(df, 'tweet')\n",
    "#df_rpunc = remove_punctuation(df, 'tweet')\n",
    "#df_rsym = remove_symbols(df, 'tweet')\n",
    "#df_rpsc = remove_punctuation_and_sc(df, 'tweet')\n",
    "#df_rrw = remove_redundant_whitespace(df, 'tweet')\n",
    "#df_rmh = remove_mentions_hashtags(df, 'tweet')\n",
    "#df_rhl = remove_hyperlink(df, 'tweet')\n",
    "#df_preprocessed = preprocessing(df, 'tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14661190",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ec37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing tweets and label for each respective preprocessing steps (to be compared by using evaluation metrics)\n",
    "\n",
    "#X = df['tweet']\n",
    "#y = df['label']\n",
    "\n",
    "#X = df_cf['tweet']\n",
    "#y = df_cf['label']\n",
    "\n",
    "#X = df_rsw['tweet']\n",
    "#y = df_rsw['label']\n",
    "\n",
    "#X = df_stem['tweet']\n",
    "#y = df_stem['label']\n",
    "\n",
    "#X = df_rpunc['tweet']\n",
    "#y = df_rpunc['label']\n",
    "\n",
    "#X = df_rsym['tweet']\n",
    "#y = df_rsym['label']\n",
    "\n",
    "#X = df_rpsc['tweet']\n",
    "#y = df_rpsc['label']\n",
    "\n",
    "#X = df_rrw['tweet']\n",
    "#y = df_rrw['label']\n",
    "\n",
    "#X = df_rmh['tweet']\n",
    "#y = df_rmh['label']\n",
    "\n",
    "#X = df_rhl['tweet']\n",
    "#y = df_rhl['label']\n",
    "\n",
    "#X = df_preprocessed['tweet']\n",
    "#y = df_preprocessed['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eaa751",
   "metadata": {},
   "source": [
    "## No Imbalanced Class Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff80f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1350ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c11cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "max_length = max([len(s.split()) for s in X])\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_val = pad_sequences(X_val, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c65018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimension of the embeddings\n",
    "dim = fasttext_model.get_dimension()\n",
    "\n",
    "# create the embedding matrix\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = fasttext_model.get_word_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84572949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002437</td>\n",
       "      <td>-0.043058</td>\n",
       "      <td>-0.018707</td>\n",
       "      <td>0.128495</td>\n",
       "      <td>-0.018542</td>\n",
       "      <td>-0.101053</td>\n",
       "      <td>-0.034554</td>\n",
       "      <td>0.143302</td>\n",
       "      <td>-0.066489</td>\n",
       "      <td>-0.113288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069537</td>\n",
       "      <td>-0.042619</td>\n",
       "      <td>-0.024802</td>\n",
       "      <td>-0.008546</td>\n",
       "      <td>-0.021058</td>\n",
       "      <td>-0.034511</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>-0.020076</td>\n",
       "      <td>-0.057329</td>\n",
       "      <td>0.047373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006226</td>\n",
       "      <td>-0.039082</td>\n",
       "      <td>-0.188137</td>\n",
       "      <td>0.121136</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.017565</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>0.010326</td>\n",
       "      <td>-0.003909</td>\n",
       "      <td>-0.072086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>-0.125213</td>\n",
       "      <td>0.051104</td>\n",
       "      <td>-0.055601</td>\n",
       "      <td>-0.034176</td>\n",
       "      <td>-0.043827</td>\n",
       "      <td>-0.025502</td>\n",
       "      <td>-0.045835</td>\n",
       "      <td>-0.027698</td>\n",
       "      <td>0.121965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.051436</td>\n",
       "      <td>-0.028671</td>\n",
       "      <td>-0.022541</td>\n",
       "      <td>0.191665</td>\n",
       "      <td>-0.029133</td>\n",
       "      <td>-0.184469</td>\n",
       "      <td>0.045916</td>\n",
       "      <td>0.121244</td>\n",
       "      <td>0.051155</td>\n",
       "      <td>-0.261428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032695</td>\n",
       "      <td>-0.007114</td>\n",
       "      <td>0.101565</td>\n",
       "      <td>0.101271</td>\n",
       "      <td>0.069635</td>\n",
       "      <td>-0.078312</td>\n",
       "      <td>0.066119</td>\n",
       "      <td>-0.240654</td>\n",
       "      <td>-0.107772</td>\n",
       "      <td>0.228045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039191</td>\n",
       "      <td>-0.041498</td>\n",
       "      <td>-0.063466</td>\n",
       "      <td>0.100154</td>\n",
       "      <td>-0.048755</td>\n",
       "      <td>-0.259327</td>\n",
       "      <td>-0.061933</td>\n",
       "      <td>-0.004503</td>\n",
       "      <td>0.026468</td>\n",
       "      <td>-0.137056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026037</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.079056</td>\n",
       "      <td>-0.005207</td>\n",
       "      <td>0.022232</td>\n",
       "      <td>-0.019059</td>\n",
       "      <td>-0.042259</td>\n",
       "      <td>-0.080854</td>\n",
       "      <td>-0.021680</td>\n",
       "      <td>0.169146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23680</th>\n",
       "      <td>0.011723</td>\n",
       "      <td>0.061613</td>\n",
       "      <td>-0.031122</td>\n",
       "      <td>0.046441</td>\n",
       "      <td>0.031312</td>\n",
       "      <td>-0.056365</td>\n",
       "      <td>-0.029583</td>\n",
       "      <td>0.016447</td>\n",
       "      <td>0.030902</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008631</td>\n",
       "      <td>-0.019576</td>\n",
       "      <td>-0.059615</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>-0.001603</td>\n",
       "      <td>-0.054475</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.052755</td>\n",
       "      <td>-0.002003</td>\n",
       "      <td>0.057023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23681</th>\n",
       "      <td>-0.025238</td>\n",
       "      <td>0.014615</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.011921</td>\n",
       "      <td>-0.003852</td>\n",
       "      <td>-0.050046</td>\n",
       "      <td>-0.008946</td>\n",
       "      <td>-0.045779</td>\n",
       "      <td>-0.030470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029670</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>-0.035793</td>\n",
       "      <td>-0.002349</td>\n",
       "      <td>0.016751</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.027547</td>\n",
       "      <td>-0.018048</td>\n",
       "      <td>-0.048202</td>\n",
       "      <td>-0.011137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23682</th>\n",
       "      <td>-0.011442</td>\n",
       "      <td>-0.030182</td>\n",
       "      <td>0.025152</td>\n",
       "      <td>0.030010</td>\n",
       "      <td>-0.036651</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>-0.007530</td>\n",
       "      <td>-0.041400</td>\n",
       "      <td>0.014827</td>\n",
       "      <td>-0.066604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016342</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>-0.002522</td>\n",
       "      <td>-0.029678</td>\n",
       "      <td>-0.007912</td>\n",
       "      <td>-0.007289</td>\n",
       "      <td>0.048508</td>\n",
       "      <td>-0.065911</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.013772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23683</th>\n",
       "      <td>-0.024868</td>\n",
       "      <td>0.088420</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.129497</td>\n",
       "      <td>-0.136866</td>\n",
       "      <td>0.021089</td>\n",
       "      <td>-0.103722</td>\n",
       "      <td>0.031331</td>\n",
       "      <td>0.137365</td>\n",
       "      <td>-0.115415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090035</td>\n",
       "      <td>0.075169</td>\n",
       "      <td>-0.143936</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>-0.108176</td>\n",
       "      <td>0.055342</td>\n",
       "      <td>-0.028520</td>\n",
       "      <td>-0.065496</td>\n",
       "      <td>0.151506</td>\n",
       "      <td>0.125149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23684</th>\n",
       "      <td>-0.010866</td>\n",
       "      <td>-0.019881</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.065581</td>\n",
       "      <td>0.030249</td>\n",
       "      <td>-0.048798</td>\n",
       "      <td>-0.018606</td>\n",
       "      <td>0.029834</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>-0.098323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023642</td>\n",
       "      <td>0.033683</td>\n",
       "      <td>-0.035517</td>\n",
       "      <td>0.008968</td>\n",
       "      <td>0.039330</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.024102</td>\n",
       "      <td>-0.136492</td>\n",
       "      <td>-0.015498</td>\n",
       "      <td>0.036063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23685 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1      0.002437 -0.043058 -0.018707  0.128495 -0.018542 -0.101053 -0.034554   \n",
       "2      0.006226 -0.039082 -0.188137  0.121136 -0.001375 -0.017565  0.021986   \n",
       "3      0.051436 -0.028671 -0.022541  0.191665 -0.029133 -0.184469  0.045916   \n",
       "4     -0.039191 -0.041498 -0.063466  0.100154 -0.048755 -0.259327 -0.061933   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23680  0.011723  0.061613 -0.031122  0.046441  0.031312 -0.056365 -0.029583   \n",
       "23681 -0.025238  0.014615  0.002253  0.030075  0.011921 -0.003852 -0.050046   \n",
       "23682 -0.011442 -0.030182  0.025152  0.030010 -0.036651  0.007234 -0.007530   \n",
       "23683 -0.024868  0.088420  0.243243  0.129497 -0.136866  0.021089 -0.103722   \n",
       "23684 -0.010866 -0.019881  0.050228  0.065581  0.030249 -0.048798 -0.018606   \n",
       "\n",
       "            7         8         9    ...       290       291       292  \\\n",
       "0      0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1      0.143302 -0.066489 -0.113288  ... -0.069537 -0.042619 -0.024802   \n",
       "2      0.010326 -0.003909 -0.072086  ...  0.007077 -0.125213  0.051104   \n",
       "3      0.121244  0.051155 -0.261428  ...  0.032695 -0.007114  0.101565   \n",
       "4     -0.004503  0.026468 -0.137056  ... -0.026037  0.006308  0.079056   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23680  0.016447  0.030902  0.011736  ...  0.008631 -0.019576 -0.059615   \n",
       "23681 -0.008946 -0.045779 -0.030470  ... -0.029670  0.030701 -0.035793   \n",
       "23682 -0.041400  0.014827 -0.066604  ... -0.016342  0.007197 -0.002522   \n",
       "23683  0.031331  0.137365 -0.115415  ... -0.090035  0.075169 -0.143936   \n",
       "23684  0.029834  0.007224 -0.098323  ... -0.023642  0.033683 -0.035517   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1     -0.008546 -0.021058 -0.034511  0.009614 -0.020076 -0.057329  0.047373  \n",
       "2     -0.055601 -0.034176 -0.043827 -0.025502 -0.045835 -0.027698  0.121965  \n",
       "3      0.101271  0.069635 -0.078312  0.066119 -0.240654 -0.107772  0.228045  \n",
       "4     -0.005207  0.022232 -0.019059 -0.042259 -0.080854 -0.021680  0.169146  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "23680  0.000401 -0.001603 -0.054475  0.006014  0.052755 -0.002003  0.057023  \n",
       "23681 -0.002349  0.016751  0.002381  0.027547 -0.018048 -0.048202 -0.011137  \n",
       "23682 -0.029678 -0.007912 -0.007289  0.048508 -0.065911  0.002160  0.013772  \n",
       "23683  0.024632 -0.108176  0.055342 -0.028520 -0.065496  0.151506  0.125149  \n",
       "23684  0.008968  0.039330  0.003242  0.024102 -0.136492 -0.015498  0.036063  \n",
       "\n",
       "[23685 rows x 300 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "764f1c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "912/912 [==============================] - 34s 35ms/step - loss: 0.3774 - val_loss: 0.3887\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 40s 43ms/step - loss: 0.2936 - val_loss: 0.3049\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 39s 43ms/step - loss: 0.2606 - val_loss: 0.3066\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 39s 42ms/step - loss: 0.2435 - val_loss: 0.2595\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 38s 42ms/step - loss: 0.2182 - val_loss: 0.2600\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 38s 42ms/step - loss: 0.2029 - val_loss: 0.2310\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 38s 42ms/step - loss: 0.1903 - val_loss: 0.2231\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 39s 43ms/step - loss: 0.1680 - val_loss: 0.2255\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 39s 42ms/step - loss: 0.1582 - val_loss: 0.2180\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 39s 42ms/step - loss: 0.1385 - val_loss: 0.2416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e63b430d00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d545c259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 15ms/step\n",
      "[[1517  117]\n",
      " [  58  334]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95      1634\n",
      "           1       0.74      0.85      0.79       392\n",
      "\n",
      "    accuracy                           0.91      2026\n",
      "   macro avg       0.85      0.89      0.87      2026\n",
      "weighted avg       0.92      0.91      0.92      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66eae9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = './suicide_detection_no_treatment.h5'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7bb135",
   "metadata": {},
   "source": [
    "# Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70614288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels and their frequencies in the training data\n",
    "class_labels = np.unique(y_train)\n",
    "class_freq = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train)\n",
    "\n",
    "# Calculate the inverse frequency as the class weight\n",
    "class_weight = dict(zip(class_labels, class_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8afe9fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "912/912 [==============================] - 35s 35ms/step - loss: 0.4927 - accuracy: 0.7528 - val_loss: 0.3930 - val_accuracy: 0.8446\n",
      "Epoch 2/10\n",
      "912/912 [==============================] - 39s 42ms/step - loss: 0.3862 - accuracy: 0.8210 - val_loss: 0.3072 - val_accuracy: 0.8816\n",
      "Epoch 3/10\n",
      "912/912 [==============================] - 31s 34ms/step - loss: 0.3577 - accuracy: 0.8317 - val_loss: 0.5463 - val_accuracy: 0.7879\n",
      "Epoch 4/10\n",
      "912/912 [==============================] - 39s 43ms/step - loss: 0.3233 - accuracy: 0.8508 - val_loss: 0.4174 - val_accuracy: 0.7990\n",
      "Epoch 5/10\n",
      "912/912 [==============================] - 43s 47ms/step - loss: 0.2934 - accuracy: 0.8703 - val_loss: 0.2615 - val_accuracy: 0.8816\n",
      "Epoch 6/10\n",
      "912/912 [==============================] - 43s 47ms/step - loss: 0.2662 - accuracy: 0.8814 - val_loss: 0.3409 - val_accuracy: 0.8594\n",
      "Epoch 7/10\n",
      "912/912 [==============================] - 43s 47ms/step - loss: 0.2458 - accuracy: 0.8973 - val_loss: 0.3763 - val_accuracy: 0.8237\n",
      "Epoch 8/10\n",
      "912/912 [==============================] - 41s 45ms/step - loss: 0.2289 - accuracy: 0.8960 - val_loss: 0.3035 - val_accuracy: 0.8742\n",
      "Epoch 9/10\n",
      "912/912 [==============================] - 40s 44ms/step - loss: 0.2068 - accuracy: 0.9126 - val_loss: 0.2903 - val_accuracy: 0.8792\n",
      "Epoch 10/10\n",
      "912/912 [==============================] - 41s 45ms/step - loss: 0.1989 - accuracy: 0.9144 - val_loss: 0.3131 - val_accuracy: 0.8742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e640e46f70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8, validation_split=0.1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae439115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 16ms/step\n",
      "[[1423  211]\n",
      " [  32  360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92      1634\n",
      "           1       0.63      0.92      0.75       392\n",
      "\n",
      "    accuracy                           0.88      2026\n",
      "   macro avg       0.80      0.89      0.83      2026\n",
      "weighted avg       0.91      0.88      0.89      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecd55dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = './suicide_detection_cw_final.h5'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1077f",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bde87",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cf44b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "adasyn = ADASYN()\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7015810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1443/1443 [==============================] - 54s 36ms/step - loss: 0.5452 - accuracy: 0.7155 - val_loss: 0.9721 - val_accuracy: 0.3920\n",
      "Epoch 2/10\n",
      "1443/1443 [==============================] - 58s 40ms/step - loss: 0.4235 - accuracy: 0.8060 - val_loss: 0.5965 - val_accuracy: 0.6454\n",
      "Epoch 3/10\n",
      "1443/1443 [==============================] - 70s 48ms/step - loss: 0.3663 - accuracy: 0.8398 - val_loss: 0.1479 - val_accuracy: 0.9626\n",
      "Epoch 4/10\n",
      "1443/1443 [==============================] - 68s 47ms/step - loss: 0.3182 - accuracy: 0.8607 - val_loss: 0.3582 - val_accuracy: 0.8168\n",
      "Epoch 5/10\n",
      "1443/1443 [==============================] - 67s 46ms/step - loss: 0.2861 - accuracy: 0.8768 - val_loss: 0.2310 - val_accuracy: 0.8963\n",
      "Epoch 6/10\n",
      "1443/1443 [==============================] - 66s 46ms/step - loss: 0.2579 - accuracy: 0.8879 - val_loss: 0.3576 - val_accuracy: 0.8270\n",
      "Epoch 7/10\n",
      "1443/1443 [==============================] - 66s 46ms/step - loss: 0.2300 - accuracy: 0.9042 - val_loss: 0.3141 - val_accuracy: 0.8597\n",
      "Epoch 8/10\n",
      "1443/1443 [==============================] - 66s 46ms/step - loss: 0.2032 - accuracy: 0.9183 - val_loss: 0.2289 - val_accuracy: 0.8979\n",
      "Epoch 9/10\n",
      "1443/1443 [==============================] - 66s 46ms/step - loss: 0.1874 - accuracy: 0.9239 - val_loss: 0.2471 - val_accuracy: 0.8924\n",
      "Epoch 10/10\n",
      "1443/1443 [==============================] - 64s 45ms/step - loss: 0.1681 - accuracy: 0.9343 - val_loss: 0.1458 - val_accuracy: 0.9462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e62f0aaf40>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_adasyn, y_adasyn, epochs=10, batch_size=8, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0252dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 14ms/step\n",
      "[[1376  258]\n",
      " [  46  346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90      1634\n",
      "           1       0.57      0.88      0.69       392\n",
      "\n",
      "    accuracy                           0.85      2026\n",
      "   macro avg       0.77      0.86      0.80      2026\n",
      "weighted avg       0.89      0.85      0.86      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49e52a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = './suicide_detection_adasyn.h5'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf46080",
   "metadata": {},
   "source": [
    "### SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a161b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoteenn = SMOTEENN()\n",
    "X_smoteenn, y_smoteenn = smoteenn.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c10644c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1042/1042 [==============================] - 50s 45ms/step - loss: 0.5010 - accuracy: 0.7499 - val_loss: 0.4938 - val_accuracy: 0.7192\n",
      "Epoch 2/10\n",
      "1042/1042 [==============================] - 46s 44ms/step - loss: 0.3663 - accuracy: 0.8386 - val_loss: 0.3293 - val_accuracy: 0.8445\n",
      "Epoch 3/10\n",
      "1042/1042 [==============================] - 46s 44ms/step - loss: 0.3093 - accuracy: 0.8660 - val_loss: 0.1823 - val_accuracy: 0.9320\n",
      "Epoch 4/10\n",
      "1042/1042 [==============================] - 47s 45ms/step - loss: 0.2691 - accuracy: 0.8856 - val_loss: 0.2570 - val_accuracy: 0.8909\n",
      "Epoch 5/10\n",
      "1042/1042 [==============================] - 47s 45ms/step - loss: 0.2345 - accuracy: 0.9015 - val_loss: 0.1716 - val_accuracy: 0.9266\n",
      "Epoch 6/10\n",
      "1042/1042 [==============================] - 43s 41ms/step - loss: 0.2133 - accuracy: 0.9160 - val_loss: 0.1412 - val_accuracy: 0.9395\n",
      "Epoch 7/10\n",
      "1042/1042 [==============================] - 38s 37ms/step - loss: 0.1878 - accuracy: 0.9274 - val_loss: 0.0587 - val_accuracy: 0.9752\n",
      "Epoch 8/10\n",
      "1042/1042 [==============================] - 39s 37ms/step - loss: 0.1713 - accuracy: 0.9344 - val_loss: 0.1488 - val_accuracy: 0.9330\n",
      "Epoch 9/10\n",
      "1042/1042 [==============================] - 35s 34ms/step - loss: 0.1506 - accuracy: 0.9413 - val_loss: 0.1869 - val_accuracy: 0.9147\n",
      "Epoch 10/10\n",
      "1042/1042 [==============================] - 44s 42ms/step - loss: 0.1377 - accuracy: 0.9441 - val_loss: 0.1077 - val_accuracy: 0.9525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e659687d90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_smoteenn, y_smoteenn, epochs=10, batch_size=8, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e7471c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 17ms/step\n",
      "[[1228  406]\n",
      " [  43  349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.85      1634\n",
      "           1       0.46      0.89      0.61       392\n",
      "\n",
      "    accuracy                           0.78      2026\n",
      "   macro avg       0.71      0.82      0.73      2026\n",
      "weighted avg       0.87      0.78      0.80      2026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_pred = np.round(y_pred)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
