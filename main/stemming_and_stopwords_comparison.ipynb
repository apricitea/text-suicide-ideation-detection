{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b525455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from hunspell import Hunspell\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e55a2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = \"C:/Code/Github/text_suicide_detection/data/final_dataset.xlsx\"\n",
    "df = pd.read_excel(path, sheet_name=\"10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de5f2d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 23:27:29 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2024-01-22 23:27:29 INFO: Using device: cpu\n",
      "2024-01-22 23:27:29 INFO: Loading: tokenize\n",
      "2024-01-22 23:27:29 INFO: Loading: mwt\n",
      "2024-01-22 23:27:29 INFO: Loading: pos\n",
      "2024-01-22 23:27:29 INFO: Loading: lemma\n",
      "2024-01-22 23:27:29 INFO: Done loading processors!\n",
      "2024-01-22 23:27:29 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2024-01-22 23:27:29 INFO: Using device: cpu\n",
      "2024-01-22 23:27:29 INFO: Loading: tokenize\n",
      "2024-01-22 23:27:29 INFO: Loading: mwt\n",
      "2024-01-22 23:27:29 INFO: Loading: pos\n",
      "2024-01-22 23:27:29 INFO: Loading: lemma\n",
      "2024-01-22 23:27:29 INFO: Loading: depparse\n",
      "2024-01-22 23:27:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Load the Stemming tools and Indonesian stopwords from Sastrawi\n",
    "stemmer_sastrawi = StemmerFactory().create_stemmer()\n",
    "stopwords_sastrawi = StopWordRemoverFactory().get_stop_words()\n",
    "\n",
    "# Load the Stemming tools and Indonesian stopwords from Hunspell\n",
    "h = Hunspell('C:/Code/Github/text_suicide_detection/hunspell-id-main/id_ID','C:/Code/Github/text_suicide_detection/hunspell-id-main/id_ID')\n",
    "hs_stopwords = Hunspell('C:/Code/Github/text_suicide_detection/hunspell-id-main/id_ID')\n",
    "stopwords_hunspell = set([s.decode('utf-8') for s in hs_stopwords.suggest('')])\n",
    "\n",
    "# Load the Stemming tools and Indonesian stopwords from Stanza\n",
    "st = stanza.Pipeline('id', download_method=None, processors='tokenize,mwt,pos,lemma')\n",
    "nlp = stanza.Pipeline('id', download_method=None)\n",
    "doc = nlp('')\n",
    "stopwords_stanza = set([word.text for sent in doc.sentences for word in sent.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ebfc9",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d7c1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sastrawi(text):\n",
    "    return stemmer_sastrawi.stem(text)\n",
    "\n",
    "def word_hunspell(word):\n",
    "    try:\n",
    "        stems = h.stem(word)\n",
    "    except UnicodeEncodeError:\n",
    "        stems = [word]\n",
    "    \n",
    "    if len(stems) == 0:\n",
    "        output = word\n",
    "    else:\n",
    "        output = stems[0]\n",
    "    return output\n",
    "    \n",
    "def stem_hunspell(text):\n",
    "    hs_stem = [word_hunspell(word) for word in text.split()]\n",
    "    output = ' '.join(hs_stem) \n",
    "    return output\n",
    "\n",
    "def stem_stanza(text):\n",
    "    doc = st(text)\n",
    "    lemmas = [word.lemma if word.lemma is not None else word.text for sent in doc.sentences for word in sent.words]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c99063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Stemming Sastrawi: main kebun sama dia saya sedang tunggu api padam hingga senja jemput\n",
      "Hasil Stemming Hunspell: main kebun sama dia saya sedang tunggu api padam hingga senja jemput\n",
      "Hasil Stemming Stanza: main kebun bersama dia saya sedang tunggu perapian padam hingga senja menjemput\n"
     ]
    }
   ],
   "source": [
    "text = 'bermain berkebun bersama dia saya sedang menunggu perapian padam hingga senja menjemput'\n",
    "\n",
    "print(f'Hasil Stemming Sastrawi: {stem_sastrawi(text)}')\n",
    "print(f'Hasil Stemming Hunspell: {stem_hunspell(text)}')\n",
    "print(f'Hasil Stemming Stanza: {stem_stanza(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48ed5b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1535.472715139389\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "# Apply the stopword removal functions to the DataFrame\n",
    "df['text_sastrawi'] = df['tweet'].apply(stem_sastrawi)\n",
    "print(\"Elapsed time:\", time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4aae256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.3992314338684082\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "# Apply the stopword removal functions to the DataFrame\n",
    "df['text_hunspell'] = df['tweet'].apply(stem_hunspell)\n",
    "print(\"Elapsed time:\", time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7b575f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 741.537252664566\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "# Apply the stopword removal functions to the DataFrame\n",
    "df['text_stanza'] = df['tweet'].apply(stem_stanza)\n",
    "print(\"Elapsed time:\", time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78246962",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'stemmed.xlsx'  \n",
    "sheetname = 'stemmed_result' \n",
    "df.to_excel(filename, sheet_name=sheetname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560e7cc",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "764b5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_sastrawi(text):\n",
    "    words = text.split()\n",
    "    words_filtered = [word for word in words if not word in stopwords_sastrawi]\n",
    "    return \" \".join(words_filtered)\n",
    "\n",
    "def remove_stopwords_hunspell(text):\n",
    "    words = text.split()\n",
    "    words_filtered = [word for word in words if not word in stopwords_hunspell]\n",
    "    return \" \".join(words_filtered)\n",
    "\n",
    "def remove_stopwords_stanza(text):\n",
    "    doc = nlp(text)\n",
    "    words_filtered = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if not word.text in stopwords_stanza:\n",
    "                words_filtered.append(word.text)\n",
    "    return \" \".join(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fa6c4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Stopwords Removal Sastrawi: aku sama mau pergi pohon mangga pulang pergi ragunan\n",
      "Hasil Stopwords Removal Hunspell: aku sama dia mau yang pergi ke pohon mangga sampai pulang pergi di ragunan\n",
      "Hasil Stopwords Removal Stanza: aku sama dia mau yang pergi ke pohon mangga sampai pulang pergi di ragunan\n"
     ]
    }
   ],
   "source": [
    "text = \"aku sama dia mau yang pergi ke pohon mangga sampai pulang pergi di ragunan\"\n",
    "\n",
    "print(f'Hasil Stopwords Removal Sastrawi: {remove_stopwords_sastrawi(text)}')\n",
    "print(f'Hasil Stopwords Removal Hunspell: {remove_stopwords_hunspell(text)}')\n",
    "print(f'Hasil Stopwords Removal Stanza: {remove_stopwords_stanza(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ecd85e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.31885337829589844\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "# Apply the stopword removal functions to the DataFrame\n",
    "df['text_sastrawi'] = df['tweet'].apply(remove_stopwords_sastrawi)\n",
    "print(\"Elapsed time:\", time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "423c9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.03647923469543457\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "# Apply the stopword removal functions to the DataFrame\n",
    "df['text_hunspell'] = df['tweet'].apply(remove_stopwords_hunspell)\n",
    "print(\"Elapsed time:\", time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14c366e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1373.5758798122406\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "# Apply the stopword removal functions to the DataFrame\n",
    "df['text_stanza'] = df['tweet'].apply(remove_stopwords_stanza)\n",
    "print(\"Elapsed time:\", time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0aa037d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'stopwords.xlsx'  \n",
    "sheetname = 'stopwords_removal_result'      \n",
    "df.to_excel(filename, sheet_name=sheetname, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
